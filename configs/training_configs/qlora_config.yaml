lora_config:
  r: 64
  lora_alpha: 16
  target_modules:
    - q_proj
    - v_proj
    - k_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

training:
  num_epochs: 3
  batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 2.0e-4
  warmup_steps: 100
  max_grad_norm: 1.0
  fp16: true
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  output_dir: "./checkpoints/qlora"

quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true
